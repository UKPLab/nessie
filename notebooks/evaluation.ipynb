{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In order to evaluate the performance of different methods and hyper-parameters, we leverage several metrics.\n",
    "As described before, we differentiate between two kinds of annotation error detectors, *flagger* and *scorer*.\n",
    "These need different metrics during evaluation, similar to classification and ranking or unranked and ranked evaluation from information retrieval.\n",
    "As flagging is a binary classification task, we use the standard metrics for this task which are precision, recall, and  F1.\n",
    "We also record the percentage of instances flagged .\n",
    "Scorer produce a ranking as seen in information retrieval.\n",
    "We use average precision (AP, also known as Area Under the Precision-Recall Curve (AUPR/AUPRC). In AED, AP is also identical to mean average precision (mAP) used in other works.) , Precision@10%, and Recall@10%.\n",
    "There are reasons why both precision and recall can be considered the more important metric of the two.\n",
    "A low precision leads to increased cost because many more instances than necessary need to be inspected manually after detection.\n",
    "Similarly, a low recall leads to problems because there still can be errors left after the application of AED.\n",
    "As both arguments have merit, we will mainly use the aggregated metrics F1 and AP.\n",
    "Precision and recall at 10% evaluate a scenario in which a scorer was applied and the first 10%  with the highest score (most likely to be wrongly annotated) are manually corrected.\n",
    "\n",
    "In contrast to other works, we explicitly do not use ROC AUC and discourage its use for AED, as it heavily overestimates performance when applied to imbalanced datasets.\n",
    "Datasets needing AED are typically very imbalanced because there are far more correct labels than wrong ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Flaggers\n",
    "\n",
    "Evaluating flaggers is similar to evaluating classification.To evaluate flagger, we use precision, recall, F1, and % of instances flagged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from nessie.detectors import Retag\n",
    "from nessie.dataloader import load_example_text_classification_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.text import FastTextTextClassifier\n",
    "from nessie.metrics import percentage_flagged_score\n",
    "\n",
    "ds = load_example_text_classification_data()\n",
    "\n",
    "model = FastTextTextClassifier()\n",
    "detector = Retag()\n",
    "\n",
    "# Running AED\n",
    "cv = CrossValidationHelper(n_splits=3)\n",
    "\n",
    "cv_result = cv.run(ds.texts, ds.noisy_labels, model)\n",
    "predicted_flags = detector.score(ds.noisy_labels, cv_result.predictions)\n",
    "\n",
    "# Evaluation\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ds.flags, predicted_flags, average=\"binary\")\n",
    "percent_flagged = percentage_flagged_score(ds.flags, predicted_flags)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"% flagged: {percent_flagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Token labeling\n",
    "\n",
    "In order to evaluate token labeling, we first flatten the ragged flags and then evaluate similarly to text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from nessie.detectors import Retag\n",
    "from nessie.dataloader import load_example_token_labeling_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.tagging import CrfSequenceTagger\n",
    "from nessie.metrics import percentage_flagged_score\n",
    "\n",
    "ds = load_example_token_labeling_data().subset(100)\n",
    "ds_flat = ds.flatten()\n",
    "\n",
    "model = CrfSequenceTagger()\n",
    "detector = Retag()\n",
    "\n",
    "# Running AED\n",
    "cv = CrossValidationHelper(n_splits=3)\n",
    "\n",
    "cv_result = cv. run_for_ragged(ds.sentences, ds.noisy_labels, model)\n",
    "cv_result_flat = cv_result.flatten()\n",
    "\n",
    "predicted_flags_flat = detector.score(ds_flat.noisy_labels, cv_result_flat.predictions)\n",
    "\n",
    "# Evaluation\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ds_flat.flags, predicted_flags_flat, average=\"binary\")\n",
    "percent_flagged = percentage_flagged_score(ds_flat.flags, predicted_flags_flat)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"% flagged: {percent_flagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sequence labeling\n",
    "\n",
    "In order to evaluate sequence labeling, we align and aggregate predictions to have a list of spans and then evaluate similarly to text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from nessie.detectors import Retag\n",
    "from nessie.dataloader import load_example_span_classification_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.tagging import CrfSequenceTagger\n",
    "from nessie.task_support.span_labeling import align_span_labeling_result, align_span_labeling_data\n",
    "\n",
    "# Load and align data\n",
    "ds = load_example_span_classification_data().subset(100)\n",
    "aligned_data = align_span_labeling_data(ds.sentences, ds.gold_labels, ds.noisy_labels)\n",
    "\n",
    "model = CrfSequenceTagger()\n",
    "detector = Retag()\n",
    "\n",
    "# Running AED\n",
    "cv = CrossValidationHelper(n_splits=3)\n",
    "\n",
    "cv_result = cv.run_for_ragged(ds.sentences, ds.noisy_labels, model)\n",
    "\n",
    "# We extract spans from BIO tags, align them with model predictions and \n",
    "# aggregate token level probabilities to span level\n",
    "cv_result_aligned = align_span_labeling_result(ds.noisy_labels, cv_result)\n",
    "\n",
    "predicted_flags_aligned = detector.score(cv_result_aligned.labels, cv_result_aligned.predictions)\n",
    "\n",
    "# Evaluation\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(aligned_data.flags, predicted_flags_aligned, average=\"binary\")\n",
    "percent_flagged = percentage_flagged_score(aligned_data.flags, predicted_flags_aligned)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"% flagged: {percent_flagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Scorers\n",
    "\n",
    "Evaluating scorers is similar to evaluating ranked retrieval in Information Retrieval. To evaluate flagger, we use precision@10, recall@10, and average precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ireval\n",
    "\n",
    "from nessie.detectors import ClassificationUncertainty\n",
    "from nessie.dataloader import load_example_text_classification_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.text import FastTextTextClassifier\n",
    "\n",
    "ds = load_example_text_classification_data()\n",
    "\n",
    "model = FastTextTextClassifier()\n",
    "detector = ClassificationUncertainty()\n",
    "\n",
    "# Running AED\n",
    "cv = CrossValidationHelper(n_splits=3)\n",
    "\n",
    "cv_result = cv.run(ds.texts, ds.noisy_labels, model)\n",
    "scores = detector.score(ds.noisy_labels, cv_result.probabilities, cv_result.le)\n",
    "\n",
    "# Evaluation\n",
    "precision_at_10_percent = ireval.precision_at_k_percent(ds.flags, scores, 10)\n",
    "recall_at_10_percent = ireval.recall_at_k_percent(ds.flags, scores, 10)\n",
    "ap = ireval.average_precision(ds.flags, scores)\n",
    "\n",
    "\n",
    "print(f\"Precision@10%: {precision_at_10_percent}\")\n",
    "print(f\"Recall@10%: {recall_at_10_percent}\")\n",
    "print(f\"Average precision: {ap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Token labeling\n",
    "\n",
    "In order to evaluate token labeling, we first flatten the ragged flags and then evaluate similarly to text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ireval\n",
    "\n",
    "from nessie.detectors import ClassificationUncertainty\n",
    "from nessie.dataloader import load_example_token_labeling_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.tagging import CrfSequenceTagger\n",
    "\n",
    "ds = load_example_token_labeling_data().subset(100)\n",
    "ds_flat = ds.flatten()\n",
    "\n",
    "model = CrfSequenceTagger()\n",
    "detector = ClassificationUncertainty()\n",
    "\n",
    "# Running AED\n",
    "cv = CrossValidationHelper(n_splits=3)\n",
    "\n",
    "cv_result = cv. run_for_ragged(ds.sentences, ds.noisy_labels, model)\n",
    "cv_result_flat = cv_result.flatten()\n",
    "\n",
    "scores_flat = detector.score(ds_flat.noisy_labels, cv_result_flat.probabilities, cv_result.le)\n",
    "\n",
    "# Evaluation\n",
    "precision_at_10_percent = ireval.precision_at_k_percent(ds_flat.flags, scores_flat, 10)\n",
    "recall_at_10_percent = ireval.recall_at_k_percent(ds_flat.flags, scores_flat, 10)\n",
    "ap = ireval.average_precision(ds_flat.flags, scores_flat)\n",
    "\n",
    "print(f\"Precision@10%: {precision_at_10_percent}\")\n",
    "print(f\"Recall@10%: {recall_at_10_percent}\")\n",
    "print(f\"Average precision: {ap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sequence labeling\n",
    "\n",
    "In order to evaluate sequence labeling, we align and aggregate predictions to have a list of spans and then evaluate similarly to text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ireval\n",
    "\n",
    "from nessie.detectors import ClassificationUncertainty\n",
    "from nessie.dataloader import load_example_span_classification_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.tagging import CrfSequenceTagger\n",
    "from nessie.task_support.span_labeling import align_span_labeling_result, align_span_labeling_data\n",
    "\n",
    "# Load and align data\n",
    "ds = load_example_span_classification_data().subset(100)\n",
    "aligned_data = align_span_labeling_data(ds.sentences, ds.gold_labels, ds.noisy_labels)\n",
    "\n",
    "model = CrfSequenceTagger()\n",
    "detector = ClassificationUncertainty()\n",
    "\n",
    "# Running AED\n",
    "cv = CrossValidationHelper(n_splits=3)\n",
    "\n",
    "cv_result = cv.run_for_ragged(ds.sentences, ds.noisy_labels, model)\n",
    "\n",
    "# We extract spans from BIO tags, align them with model predictions and \n",
    "# aggregate token level probabilities to span level\n",
    "cv_result_aligned = align_span_labeling_result(ds.noisy_labels, cv_result)\n",
    "\n",
    "scores_aligned = detector.score(cv_result_aligned.labels, cv_result_aligned.probabilities, cv_result_aligned.le)\n",
    "\n",
    "# Evaluation\n",
    "precision_at_10_percent = ireval.precision_at_k_percent(aligned_data.flags, scores_aligned, 10)\n",
    "recall_at_10_percent = ireval.recall_at_k_percent(aligned_data.flags, scores_aligned, 10)\n",
    "ap = ireval.average_precision(aligned_data.flags, scores_aligned)\n",
    "\n",
    "print(f\"Precision@10%: {precision_at_10_percent}\")\n",
    "print(f\"Recall@10%: {recall_at_10_percent}\")\n",
    "print(f\"Average precision: {ap}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}